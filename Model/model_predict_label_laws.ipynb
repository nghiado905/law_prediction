{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger # thư viện NLP tiếng Việt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim # thư viện NLP\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 431.49it/s]\n",
      "100%|██████████| 180/180 [00:01<00:00, 121.15it/s]\n",
      "100%|██████████| 67/67 [00:00<00:00, 119.82it/s]\n",
      "100%|██████████| 545/545 [00:01<00:00, 439.86it/s]\n",
      "100%|██████████| 474/474 [00:02<00:00, 216.24it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 291.75it/s]\n",
      "100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "dir_path = os.path.join(dir_path, 'Data')\n",
    "\n",
    "def get_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    dirs = os.listdir(folder_path)\n",
    "    for path in tqdm(dirs):\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                #xử lí các kí tự đặc biệt\n",
    "                lines = ' '.join(lines)\n",
    "                lines = ViTokenizer.tokenize(lines)\n",
    "                # tách các từ\n",
    "                X.append(lines)\n",
    "                y.append(path)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "train_path = os.path.join(dir_path, 'D:\\Private\\DATA\\DATA_LAW\\TRAIN_FULL')\n",
    "X_data, y_data = get_data(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 427.18it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 109.36it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 137.20it/s]\n",
      "100%|██████████| 234/234 [00:00<00:00, 469.17it/s]\n",
      "100%|██████████| 204/204 [00:00<00:00, 215.21it/s]\n",
      "100%|██████████| 67/67 [00:00<00:00, 262.65it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(dir_path, 'D:\\Private\\DATA\\DATA_LAW\\TEST_FULL')\n",
    "X_test, y_test = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(data,file_path):\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "save_to_file(X_data,'D://Private//DATA//Model_data//x_train_data.txt')\n",
    "save_to_file(y_data, 'D://Private//DATA//Model_data//y_train_data.txt')\n",
    "\n",
    "\n",
    "save_to_file(X_test,'D://Private//DATA//Model_data//x_test_data.txt')\n",
    "save_to_file(y_test, 'D://Private//DATA//Model_data//y_test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(X_data, open('D:/Private/DATA/Model_data/X_data.pkl', 'wb'))\n",
    "pickle.dump(y_data, open('D:/Private/DATA/Model_data/y_data.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(X_test, open('D:/Private/DATA/Model_data/X_test.pkl', 'wb'))\n",
    "pickle.dump(y_test, open('D:/Private/DATA/Model_data/y_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_data = pickle.load(open('D:/Private/DATA/Model_data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('D:/Private/DATA/Model_data/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('D:/Private/DATA/Model_data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('D:/Private/DATA/Model_data/y_test.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAY 1: Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_data_count = count_vect.transform(X_data)\n",
    "X_test_count = count_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 79)\t1\n",
      "  (0, 191)\t1\n",
      "  (0, 252)\t1\n",
      "  (0, 280)\t1\n",
      "  (0, 294)\t1\n",
      "  (0, 365)\t1\n",
      "  (0, 449)\t2\n",
      "  (0, 452)\t1\n",
      "  (0, 453)\t1\n",
      "  (0, 548)\t1\n",
      "  (0, 634)\t2\n",
      "  (0, 699)\t1\n",
      "  (0, 887)\t1\n",
      "  (0, 993)\t1\n",
      "  (0, 1329)\t1\n",
      "  (0, 1572)\t1\n",
      "  (0, 1685)\t1\n",
      "  (0, 1730)\t1\n",
      "  (0, 1808)\t2\n",
      "  (0, 1810)\t1\n",
      "  (0, 1812)\t1\n",
      "  (0, 1886)\t1\n",
      "  (0, 1922)\t2\n",
      "  (0, 1941)\t1\n",
      "  (0, 2038)\t1\n",
      "  :\t:\n",
      "  (721, 1606)\t11\n",
      "  (721, 1624)\t5\n",
      "  (721, 1693)\t1\n",
      "  (721, 1730)\t2\n",
      "  (721, 1941)\t2\n",
      "  (721, 2038)\t1\n",
      "  (721, 2172)\t7\n",
      "  (721, 2267)\t5\n",
      "  (721, 2268)\t2\n",
      "  (721, 2305)\t1\n",
      "  (721, 2447)\t8\n",
      "  (721, 2564)\t6\n",
      "  (721, 2628)\t2\n",
      "  (721, 2692)\t3\n",
      "  (721, 2742)\t1\n",
      "  (721, 2774)\t3\n",
      "  (721, 2875)\t1\n",
      "  (721, 2882)\t1\n",
      "  (721, 2885)\t7\n",
      "  (721, 3059)\t4\n",
      "  (721, 3071)\t1\n",
      "  (721, 3079)\t3\n",
      "  (721, 3173)\t3\n",
      "  (721, 3187)\t3\n",
      "  (721, 3285)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-Idf Vectors as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngram_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X_data)\n",
    "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " SVD (singular value decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "\n",
    "\n",
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_data_tfidf_ngram)\n",
    "\n",
    "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "word2vec_model_path = os.path.join(dir_path, \"vi.vec\")\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path)\n",
    "vocab = w2v.key_to_index\n",
    "wv = w2v\n",
    "\n",
    "def get_word2vec_data(X):\n",
    "    word2vec_data = []\n",
    "    for x in X:\n",
    "        sentence = []\n",
    "        for word in x.split(\" \"):\n",
    "            if word in vocab:\n",
    "                sentence.append(wv[word])\n",
    "\n",
    "        word2vec_data.append(sentence)\n",
    "\n",
    "    return word2vec_data\n",
    "\n",
    "X_data_w2v = get_word2vec_data(X_data)\n",
    "X_test_w2v = get_word2vec_data(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1508, 3345)\n",
      "y_train shape: (1508,)\n",
      "X_test shape: (722, 3345)\n",
      "y_test shape: (722,)\n",
      "Validation accuracy:  0.8452380952380952\n",
      "Test accuracy:  0.8601108033240997\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data, X_test, y_test = X_data_tfidf, y_data, X_test_tfidf, y_test\n",
    "classifier = LogisticRegression()\n",
    "model = train_model(classifier,X_data, y_data, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dau_thau'], dtype='<U8')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'D:\\\\Private\\\\DATA\\Model\\\\bbbb.txt'\n",
    "\n",
    "with open(input_file,'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = ViTokenizer.tokenize(text)\n",
    "text_to_vect = tfidf_vect.transform([text])\n",
    "classifier.predict(text_to_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense\n",
    "from tensorflow.keras import models, optimizers\n",
    "\n",
    "def create_lstm_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    # Reshape dữ liệu thành (10, 30)\n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    \n",
    "    # Thêm lớp LSTM\n",
    "    layer = LSTM(128, return_sequences=False)(layer)\n",
    "    \n",
    "    # Thêm các lớp Dense\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    # Lớp đầu ra\n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    # Tạo mô hình\n",
    "    classifier = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Biên dịch mô hình\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, X_test, y_test = X_data_w2v, y_data, X_test_w2v, y_test\n",
    "\n",
    "model = create_lstm_model()\n",
    "classifier = model\n",
    "train_model(classifier,X_data, y_data, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
